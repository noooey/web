# 4장 모델 훈련

# 4.1 선형 회귀

데이터 분포를 가장 잘 설명할 수 있는 최적의 **선형 모델**을 찾는 방식

일반적으로 선형 모델은 **입력 특성의 가중치 합**과 **편향**이라는 상수를 더해 예측을 만듦

$$
\hat y = \Theta_0 + \Theta_1 x_1 + \Theta_2 x_2 + \cdots + \Theta_n x_n
$$

- 선형 회귀모델의 예측
    
    $\hat y$ : 예측값
    
    $\Theta$ : 모델 파라미터
    
    - $\Theta_0$  : 편향
    - $\Theta_1 \cdots \Theta_n$ : 특성 가중치
    
    $x$ : 특성값($x_0$ = 1)
    

$$
\hat y = h_\Theta(X) = \Theta \dot \ X
$$

- 벡터형태로 나타내면
    
    $$
    \Theta \dot\ X = \begin{bmatrix}
    \Theta_0 & \Theta_1 & \cdots & \Theta_n
    \end{bmatrix} \begin{bmatrix}
    x_0 \\ x_1 \\ \vdots \\ x_n
    \end{bmatrix} = \Theta_0 + \Theta_1 x_1 + \Theta_2 x_2 + \cdots + \Theta_n x_n
    $$
    
    $h_\Theta$ : 모델 파라미터 $\Theta$를 사용한 가설함수
    

## 📌 모델 훈련

모델이 훈련 세트에 가장 잘 맞도록 **모델 파라미터**를 설정

**비용함수**를 최소화하는 **θ**값을 찾아야 함

$$
RMSE(\hat \Theta) = \sqrt{MSE(\hat \Theta)} = \sqrt{E((\hat \Theta - \Theta)^2)}
$$

$$
MSE(X, h_\Theta) = {1 \over m} \sum_{i=1}^m(\Theta^TX^{(i)} - y^{(i)})^2
$$

## 4.1.1 정규방정식

비용 함수를 최소화하는 **θ**값을 찾기 위한 **해석적인 방법**, 결과 바로 도출

$$
\hat \Theta = (X^TX)^{-1}X^Ty
$$

$X^TX$ 의 역행렬 계산 → 계산 복잡도 $O(n^3)$ ... 계산량이 많다

$X^TX$ 의 역행렬을 못 구할 때 **유사역행렬**$X^+$을 구해서 $\hat \Theta = X^+y$ 를 계산

### 📌 특잇값 분해(SVD)

표준 행렬 분해 기법

훈련 세트 행렬  $X$ 를 3개의 행렬 곱셈 $UΣV^T$  분해

$X^+ = VΣ^+U^T$  

→ 계산 복잡도 $O(n^2)$ ... 계산량이 많다

## 4.1.2 계산 복잡도

문제를 해결하는 데 필요한 시간, 공간 자원 등

시간복잡도, 공간복잡도 등을 기준으로 구함

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled.png)

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%201.png)

# 4.2 경사하강법

비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해나가는 방식

1. 파라미터 벡터**θ**에 대해 비용 함수의 현재 **gradient(기울기)**를 계산
2. gradient가 감소하는 방향으로 진행
3. gradient가 0이 되면 최솟값에 도달한 것

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%202.png)

$$
x_{i+1} = x_i - \gamma_i\nabla f(x_i)
$$

### 📌 학습률

한 번 학습할 때 얼마나 변화를 줄 지를 결정하는 하이퍼파라미터

1. 학습률이 너무 작을 때
    
    수렴까지 너무 **오랜 시간**이 걸림
    

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%203.png)

1. 학습률이 너무 클 때
    
    수렴이 아니라 **발산**을 할 수도 있음
    

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%204.png)

여러 가지 학습률에 대한 경사 하강법

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%205.png)

### 📌 경사 하강법의 문제점

- (왼쪽에서 시작한 경우) **전역 최솟값**이 아닌 **지역 최솟값**에 수렴할 수 있음
- (오른쪽에서 시작한 경우) 평지를 지나는 데 오래걸리고 일찍 멈춰 **최솟값에 도달하지 못할 수 있음**

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%206.png)

### 📌 스케일 조정

경사 하강법 사용 시 **모든 특성이 스케일이 같도록** 조정해 주어야 한다

특성들의 스케일이 다르면 수렴하는데 오래걸림

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%207.png)

### 📌 편도함수

파라미터가 변경될 때 **비용 함수의 변화량**

학습 파라미터의 변화에 대해 편도함수를 계산하여 갱신함

MSE의 편도함수

$$
{\partial \over \partial \Theta_j} MSE(\Theta) = {2 \over m} \sum_{i=1}^m(\Theta^TX^{(i)} - y^{(i)})x_i^{(i)}
$$

### 📌 배치와 에포크

`배치` 모델의 가중치를 한 번 업데이트 시킬 때 사용되는 샘플의 묶음

`에포크` 학습 횟수

![이미지 출처 [https://bskyvision.com/803](https://bskyvision.com/803)](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%208.png)

이미지 출처 [https://bskyvision.com/803](https://bskyvision.com/803)

## 4.2.1 배치 경사 하강법

전체 학습 데이터를 하나의 배치(배치크기=n)으로 묶어서 학습

$$
\nabla_\Theta MSE(\Theta) = \begin{bmatrix}
{\partial \over \partial \Theta_0} MSE(\Theta) \\ {\partial \over \partial \Theta_1} MSE(\Theta) \\ \vdots \\ {\partial \over \partial \Theta_n} MSE(\Theta)
\end{bmatrix} ={2 \over m}X^T(X\Theta-y)
$$

- 파라미터 업데이트(스텝)
    
    $\eta$ : 학습률
    

$$
\Theta^{(nextstep)} = \Theta - \eta\nabla_\Theta MSE(\Theta)
$$

- 업데이트 횟수가 가장 적음, **한 에포크에 업데이트 한 번**
- 훈련 세트가 커지면 매우 느려짐
    - 전체 학습 데이터를 하나로 묶어서 학습하기 때문에 **매번 같은 데이터**에 대해서 경사를 구하므로 **수렴이 안정적**
        
        → **지역 최저점**에 빠지는 문제가 발생할 수 있음
        

![이미치 출처 [https://skyil.tistory.com/68](https://skyil.tistory.com/68)](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%209.png)

이미치 출처 [https://skyil.tistory.com/68](https://skyil.tistory.com/68)

## 4.2.2 확률적 경사 하강법

전체 학습 데이터 중 한 개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 gradient를 계산(배치 사이즈=1)

- 학습 속도가 **빠름**
- 매우 큰 훈련 세트도 학습 가능
- 확률적이므로 배치 경사 하강법보다 훨씬 **불안정**
    
    → 심한 노이즈가 존재해 지역 최저점에서는 구해줄 수 있지만 전역 최저점에 도달하는 것도 어려움
    
    → 학습률을 처음엔 크게하고 점차 출여나가는 방식으로 해결할 수 있음
    

![이미지 출처 [https://skyil.tistory.com/68](https://skyil.tistory.com/68)](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2010.png)

이미지 출처 [https://skyil.tistory.com/68](https://skyil.tistory.com/68)

## 4.2.3 미니 배치 경사 하강법

전체 학습 데이터(n)를 m개 씩 쪼개어 한 에포크에 n/m번 경사하강법 진행(배치 사이즈=m, 하이퍼파라미터)

- 배치 사이즈에 따라 계산량 조절 가능
- 확률적 경사 하강법 보다 덜 불안함
- **적당한 노이즈**가 존재해 전역 최솟값이 아닌 지역 최솟값에 수렴할 수 있는 배치 경사 하강법의 문제를 완화

![이미지 출처 [https://skyil.tistory.com/68](https://skyil.tistory.com/68)](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2011.png)

이미지 출처 [https://skyil.tistory.com/68](https://skyil.tistory.com/68)

파라미터 공간에 표시된 경사 하강법의 경로

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2012.png)

# 4.3 다항 회귀

데이터가 비선형일 경우, 각 특성의 거듭제곱을 새로운 특성으로 추가하고, 이 확장된 특성을 포함한 데이터셋에 **선형 모델**을 훈련시키는 기법 (n차함수)

2차방정식으로 생성한 데이터를 예측하는 다항회귀 모델

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2013.png)

# 4.4 학습 곡선

위의 훈련 데이터(2차방정식으로 생성한 데이터)에 대한 학습곡선

- `선형 모델`:   데이터들에 비해서 모델이 너무 단순함 → **과소적합(언더피팅)**
- `2차 다항 회귀 모델`: **일반화**가 잘 된 모델
- `300차 다항 회귀 모델`:  데이터들에 비해서 모델이 너무 복잡함 → **과대적합(오버피팅)**

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2014.png)

### 📌 오버피팅, 언더피팅

- 오버피팅
    - 훈련 세트에 과대적합 되어 훈련 세트에 대해서는 높은 정확도를 보이지만
    - 검증 세트에 대해서는 오차가 커짐
- 언더피팅
    - 충분히 학습이 진행되지 않아 훈련 세트와 검증 세트 모두 낮은 정확도를 보임

![이미지 출처 [https://rk1993.tistory.com/entry/Ridge-regression와-Lasso-regression-쉽게-이해하기](https://rk1993.tistory.com/entry/Ridge-regression%EC%99%80-Lasso-regression-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0)](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2015.png)

이미지 출처 [https://rk1993.tistory.com/entry/Ridge-regression와-Lasso-regression-쉽게-이해하기](https://rk1993.tistory.com/entry/Ridge-regression%EC%99%80-Lasso-regression-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0)

![이미지 출처 [https://hive.blog/ml/@yskoh/mit-6-s191-1](https://hive.blog/ml/@yskoh/mit-6-s191-1)](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2016.png)

이미지 출처 [https://hive.blog/ml/@yskoh/mit-6-s191-1](https://hive.blog/ml/@yskoh/mit-6-s191-1)

### 📌 일반화 성능 추정

- 교차 검증
    - 오버피팅: 훈련 데이터에서 성능 좋지만 교차 검증 점수는 나쁨
    - 언더피팅: 양쪽 모두 나쁨
- 학습 곡선
    - 훈련세트와 검증 세트의 모델 성능을 나타냄
        
        ![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2017.png)
        
        **RMSE(오차)가 높은 부근**에서 훈련세트와 검증세트의 학습곡선이 가까이 근접해있음
        
        → 과소적합
        
        ![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2018.png)
        
        RMSE(오차)가 위의 예시에 비해서 훨씬 낮은 부근에서 훈련세트와 검증세트의 학습곡선이 가까워지지만 **두 곡선 사이의 공간**이 있고, **훈련세트의 RMSE(오차)가 더 낮음**
        
        → 과대적합
        

### 📌 편향/분산 트레이드 오프

- `편향` 잘못된 가정으로 인해 발생
- `분산` 훈련 데이터에 있는 작은 변동에 모델이 과도하게 민감해서 발생
- `줄일 수 없는 오차` 데이터 자체의 잡음에 의해 발생

`복잡도`가 커지면(고차 다항 회귀 모델) `분산`이 늘어나고(작은 변동에 대해 민감해짐) `편향`은 줄어듦

`복잡도`가 줄어들면 `편향`이 커지고 `분산`이 작아짐

### 📌 오버피팅 개선 방법

- 데이터 양 늘리기
- 모델 복잡도 줄이기(다항회귀 차수 감소)
- 규제
- 드롭아웃

# 4.5 규제가 있는 선형 모델

가중치를 제한함으로써 선형 회귀 모델 규제

## 4.5.1 릿지 회귀

$$
J(\Theta) = MSE(\Theta) + \alpha{1\over2}\sum_{i=1}^n(\Theta_i^2)
$$

- 비용함수에 규제항 추가된 형태
    
    $\alpha$ : 얼마나 많이 규제할지 조절(하이퍼 파라미터)
    

$$
\alpha{1\over2}\sum_{i=1}^n(\Theta_i^2)
$$

규제항은 **훈련하는 동안에만** 비용 함수에 추가 됨

모델 훈련 이후 **모델 성능** 평가는 **규제가 없는 성는 지표**로 평가

L2 Norm의 제곱을 2로 나눈 것을 규제항으로 사용

$$
{1\over2}(\|W\|_2)^2
$$

### 릿지 규제 적용

$\alpha$ 가 커질수록 선형에 가까워짐

→ `분산`은 줄어들고 `편향`은 커짐

![왼쪽 선형 회귀, 오른쪽 다항 회귀](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2019.png)

왼쪽 선형 회귀, 오른쪽 다항 회귀

## 4.5.2 라쏘 회귀

L1 Norm을 규제항으로 사용

$$
J(\Theta) = MSE(\Theta) + \alpha\sum_{i=1}^n|\Theta_i|
$$

### 라쏘 규제 적용

마찬가지로 $\alpha$ 가 커질수록 선형에 가까워짐

덜 중요한 특성의 가중치를 제거한다는 차이가 있음

- 자동으로 특성을 선택하고 **희소 모델**을 만듦

![왼쪽 선형 회귀, 오른쪽 다항 회귀](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2020.png)

왼쪽 선형 회귀, 오른쪽 다항 회귀

### 📌 라쏘와 릿지 비교

- L1 손실 ($|\Theta_1| + |\Theta_2|$)
    1. 두 파라미터가 동일하게 감소되다가
    2. $\Theta_2$ 가 0에 더 가까워서 먼제 0으로 도달
    3. 그 다음 $\Theta_1$ 이 0에 도달
- 라쏘($MSE +L1$ 손실) 손실 함수
    - $\Theta_2$ = 0에 먼저 수렴
    - 그 다음 $\Theta_1$ 이 축을 따라 **진동**하며 **전역 최적점 도달**
- L2 손실
    - 경사 하강법이 원점까지 **직선 경로**를 따라감
- 릿지 손실 함수
    - 파라미터가 **전역 최적점에 가까워질수록 gradient 작아짐**
    - $\alpha$ 를 증가시킬수록 최적의 파라미터가 원점에 더 가까워지만 **전역 최적점에 도달하지는 않음**

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2021.png)

### 📌 서브 그레디언트 벡터

라쏘는 $\theta_i=0$ 이 될 때 미분 불가능

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2022.png)

## 4.5.3 엘라스틱넷

릿지와 라쏘의 규제항을 더해서 규제항으로 사용

$r$ : 릿지와 라쏘의 혼합 비율

- r=0 엘라스틱넷 = 릿지 회귀
- r=1 엘라스틱넷 = 라쏘 회귀

$$
J(\Theta) = MSE(\Theta) + r\alpha\sum_{i=1}^n|\Theta_i| + {1-r\over2}\alpha\sum_{i=1}^n\Theta_i^2
$$

### 📌

- **릿지**가 기본으로 사용
- 쓰이는 특성이 **몇 개 뿐**이라고 보일 때 **라쏘**나 **엘라스틱넷**이 좋음
    
    → 불필요한 특성 가중치를 0으로 만들어 줌
    
- if (특성 수 > 훈련 샘플 수) || (특성 몇 개가 강하게 연관): **엘라스틱넷** 선호

## 4.5.4 조기 종료

검증 에러가 최솟값에 도달하면 바로 훈련 중지

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2023.png)

# 4.6 로지스틱 회귀

회귀를 사용하여 데이터가 어떤 범주에 속할 **확률을 0~1사이의 값으로 예측**하고 확률이 더 높은 범주로 **분류**해주는 지도학습알고리즘

## 4.6.1 확률 추정

- 입력 특성의 가중치 합 계산하여 **로지스틱** 출력
- 로지스틱 = **시그모이드 함수** (0~1 사이 값 출력)

$$
\sigma(t) = {1\over{1+exp(-t)}}
$$

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2024.png)

### 로지스틱 회귀 모델의 확률 추정

$$
\hat p = h_\Theta(X) = \sigma(\Theta^TX)
$$

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2025.png)

1(양성 클래스) / 0(음성 클래스)

## 4.6.2 훈련과 비용 함수

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2026.png)

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2027.png)

### 로지스틱 회귀의 비용 함수

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2028.png)

### 로지스틱 비용함수의 편도함수

- 각 샘플에 대해 오차를 계산하고
- j번째 특성값을 곱해서
- 모든 훈련 샘플에 대해 평균을 냄

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2029.png)

## 4.6.3 결정 경계

### 로지스틱 회귀 모델로 붓꽃 품종 분류하기

1. 꽃잎의 너비가 0~3cm인 꽃에 대해 모델의 추정 확률 계산
    
    ![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2030.png)
    
    - `Iris-Verginica`은 꽃잎 너비 1.4~2.5cm에 분포
        - 2cm 이상에서 `Iris-Verginica`이라고 강하게 확신
    - `Iris-Verginica 아님`은 꽃잎 너비 0.1~1.8에 분포
        - 1cm이하에서 `Iris-Verginica아님`이라고 강하게 확신
    - 1.6cm 부근에서 양쪽 확률이 50%가 되는 **결정경계** 형성
    

## 4.6.4 소프트맥스 회귀

로지스틱 회귀 모델이 **다중 클래스를 분류**하도록 일반화 된 것, 다항 로지스틱 회귀

1. 클래스 k에 대한 **소프트맥스 점수**

$$
S_k(X) = (\Theta^{(k)})^TX
$$

$\Theta^{(k)}$ : 각 클래스마다 파라미터 벡터를 지님

1. **소프트맥스 함수**를 통과시켜 클래스 k에 속할 **확률 $\hat p_k$** 를 추정

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2031.png)

1. 추정 확률이 가장 높은 클래스 선택

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2032.png)

### 크로스 엔트로피 비용함수

크로스 엔트로피 비용 함수를 최소화 → 타깃 클래스에 대해 **낮은 확률을 예측하는 모델을 억제**

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2033.png)

$y_k^{(i)}$ : i번째 샘플이 클래스 k에 속할 타깃 확률 (일반적으로 1 or 0)

### 클래스 k에 대한 크로스 엔트로피의 그레디언트 벡터

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2034.png)

### 소프트맥스 회귀로 세 품종의 붓꽃 분류

![Untitled](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20dd03aac529f14e979c7df8eebf423677/Untitled%2035.png)

- 확률이 50% 이하인 클래스도 예측될 수 있음
- 모든 결정 경게가 만나는 지점은 모든 클래스가 동일하게 33% 확률